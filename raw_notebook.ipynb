{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-24T16:27:20.369159Z",
     "start_time": "2025-11-24T16:26:29.724808Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import spacy\n",
    "from typing import List, Set, Dict\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from mistralai import Mistral\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# âš ï¸ REPLACE WITH YOUR KEY (Do not hardcode in production)\n",
    "MISTRAL_API_KEY = \"iJ9SkFczJxTpP4Wy95onu0W4pGr38QH1\"\n",
    "MODEL_NAME = \"open-mistral-7b\"\n",
    "DATA_DIR = \"data\"\n",
    "CHARACTER_MAP_FILE = \"story_characters.json\"\n",
    "\n",
    "# Initialize Clients\n",
    "if not MISTRAL_API_KEY:\n",
    "    print(\"âš ï¸ WARNING: Please set your MISTRAL_API_KEY.\")\n",
    "    client = None\n",
    "else:\n",
    "    client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "\n",
    "# Initialize SpaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading SpaCy...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "print(\"âœ… Setup Complete.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Multi-Language Invoice Extractor\\.venv1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup Complete.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:27:24.148859Z",
     "start_time": "2025-11-24T16:27:24.136809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_characters_hybrid(text, story_title):\n",
    "    \"\"\"\n",
    "    Extracts ALL characters from the full story text.\n",
    "    Combines SpaCy (Speed) + Mistral (Intellect).\n",
    "    \"\"\"\n",
    "    characters = set()\n",
    "\n",
    "    # 1. SpaCy (Fast)\n",
    "    doc = nlp(text[:100000])\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            characters.add(ent.text.strip())\n",
    "\n",
    "    # 2. Mistral (Smart) - Only run on the first 4k chars to identify main cast\n",
    "    if client:\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                Identify the distinct character names in this story.\n",
    "                Return ONLY a JSON list of strings. Example: [\"John\", \"Mary\"].\n",
    "\n",
    "                STORY: {story_title}\n",
    "                TEXT: {text[:4000]}\n",
    "                \"\"\"}\n",
    "            ]\n",
    "            resp = client.chat.complete(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            content = resp.choices[0].message.content\n",
    "            llm_chars = json.loads(content)\n",
    "\n",
    "            # Handle list or dict return\n",
    "            if isinstance(llm_chars, list): characters.update(llm_chars)\n",
    "            elif isinstance(llm_chars, dict):\n",
    "                for val in llm_chars.values():\n",
    "                    if isinstance(val, list): characters.update(val)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Extraction skipped for {story_title}: {e}\")\n",
    "\n",
    "    # Clean duplicates and short noise\n",
    "    return list({c for c in characters if len(c) > 2})"
   ],
   "id": "5ac868a450198132",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:27:43.635547Z",
     "start_time": "2025-11-24T16:27:29.223814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"ðŸ“‚ Loading documents...\")\n",
    "dir_loader = DirectoryLoader(DATA_DIR, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})\n",
    "raw_docs = dir_loader.load()\n",
    "\n",
    "# Dictionary to store { \"Story Title\": [\"Char1\", \"Char2\"] }\n",
    "global_character_map = {}\n",
    "processed_docs = []\n",
    "\n",
    "print(f\"ðŸ•µï¸  Extracting characters for {len(raw_docs)} stories...\")\n",
    "\n",
    "for doc in raw_docs:\n",
    "    full_text = doc.page_content\n",
    "    lines = full_text.split(\"\\n\")\n",
    "    # Heuristic: Assume first line is title, or filename\n",
    "    title = lines[0].strip() if lines else \"Unknown\"\n",
    "    body = clean_text(full_text)\n",
    "\n",
    "    # Extract once per story\n",
    "    story_chars = extract_characters_hybrid(body, title)\n",
    "    global_character_map[title] = story_chars\n",
    "    print(f\"   -> {title}: Found {len(story_chars)} characters.\")\n",
    "\n",
    "    # Update doc metadata (Global context)\n",
    "    doc.metadata[\"storyTitle\"] = title\n",
    "    doc.page_content = body\n",
    "    processed_docs.append(doc)\n",
    "\n",
    "# Save the Map separately\n",
    "print(f\"ðŸ’¾ Saving character map to {CHARACTER_MAP_FILE}...\")\n",
    "with open(CHARACTER_MAP_FILE, \"w\") as f:\n",
    "    json.dump(global_character_map, f, indent=4)"
   ],
   "id": "98823e0ebb812cee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading documents...\n",
      "ðŸ•µï¸  Extracting characters for 6 stories...\n",
      "   -> A Mother: Found 26 characters.\n",
      "   -> Sorrow: Found 7 characters.\n",
      "   -> The Lantern Keepers: Found 6 characters.\n",
      "   -> The Poor Relationâ€™s Story: Found 14 characters.\n",
      "   -> Mens Love: Found 22 characters.\n",
      "   -> The Schoolmistress: Found 11 characters.\n",
      "ðŸ’¾ Saving character map to story_characters.json...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:33:00.535076Z",
     "start_time": "2025-11-24T16:33:00.510046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def identify_chars_in_chunk(chunk_text: str, global_chars: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Checks which global characters appear in this specific chunk.\n",
    "    This is pure Python (Fast), no LLM calls.\n",
    "    \"\"\"\n",
    "    present = []\n",
    "    chunk_lower = chunk_text.lower()\n",
    "    for char in global_chars:\n",
    "        # Check if name exists in chunk\n",
    "        if char.lower() in chunk_lower:\n",
    "            present.append(char)\n",
    "    return present\n",
    "\n",
    "print(\"âœ‚ï¸  Splitting and enriching chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "final_chunks = text_splitter.split_documents(processed_docs)\n",
    "\n",
    "# Enrich chunks with LOCAL character data\n",
    "for chunk in final_chunks:\n",
    "    title = chunk.metadata.get(\"storyTitle\")\n",
    "    # Get the master list for this story\n",
    "    master_list = global_character_map.get(title, [])\n",
    "\n",
    "    # Find which of these are in THIS specific chunk\n",
    "    chunk_chars = identify_chars_in_chunk(chunk.page_content, master_list)\n",
    "\n",
    "    # Store in metadata\n",
    "    chunk.metadata[\"chars_in_chunk\"] = chunk_chars\n",
    "\n",
    "print(f\"âœ… Processed {len(final_chunks)} chunks. Example Metadata:\")\n",
    "print(final_chunks[0].metadata)"
   ],
   "id": "48f6f5c6deb286ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸  Splitting and enriching chunks...\n",
      "âœ… Processed 190 chunks. Example Metadata:\n",
      "{'source': 'data\\\\a-mother.txt', 'storyTitle': 'A Mother', 'chars_in_chunk': ['Hoppy Holohan', 'Mrs Kearney', 'Holohan', 'Mr Holohan']}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:33:10.694122Z",
     "start_time": "2025-11-24T16:33:00.799033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"ðŸ§  Creating Vector Index...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_db = FAISS.from_documents(final_chunks, embedding_model)\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"ðŸ”¤ Creating BM25 Index...\")\n",
    "bm25_retriever = BM25Retriever.from_documents(final_chunks)\n",
    "bm25_retriever.k = 5\n",
    "with open(\"bm25_retriever.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bm25_retriever, f)\n",
    "\n",
    "print(\"âœ… Indexing Complete!\")"
   ],
   "id": "f39652cc4d22f0dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Creating Vector Index...\n",
      "ðŸ”¤ Creating BM25 Index...\n",
      "âœ… Indexing Complete!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:34:42.631895Z",
     "start_time": "2025-11-24T16:34:42.615384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_and_validate_json(data):\n",
    "    \"\"\"\n",
    "    Enforces schema but tries to fix common LLM mistakes before rejecting data.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, dict):\n",
    "        return {\"error\": \"Invalid JSON format\"}\n",
    "\n",
    "    structured_data = {\n",
    "        \"name\": data.get(\"name\", \"Unknown\"),\n",
    "        \"storyTitle\": data.get(\"storyTitle\", \"Unknown\"),\n",
    "        \"summary\": data.get(\"summary\", \"Summary not available.\"),\n",
    "        \"relations\": [],\n",
    "        \"characterType\": data.get(\"characterType\", \"Unknown\")\n",
    "    }\n",
    "\n",
    "    # Validate Relations with Fuzzy Key Matching\n",
    "    raw_relations = data.get(\"relations\", [])\n",
    "    if isinstance(raw_relations, list):\n",
    "        for item in raw_relations:\n",
    "            if isinstance(item, dict) and \"name\" in item:\n",
    "                # FIX: Check for 'relation' OR 'relationship'\n",
    "                rel_value = item.get(\"relation\") or item.get(\"relationship\") or \"Connected\"\n",
    "\n",
    "                structured_data[\"relations\"].append({\n",
    "                    \"name\": item[\"name\"],\n",
    "                    \"relation\": rel_value\n",
    "                })\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "def get_character_info(character_name, verbose=True):\n",
    "    # 1. Load Map\n",
    "    if os.path.exists(CHARACTER_MAP_FILE):\n",
    "        with open(CHARACTER_MAP_FILE, \"r\") as f:\n",
    "            global_map = json.load(f)\n",
    "    else:\n",
    "        return {\"error\": \"Character map not found.\"}\n",
    "\n",
    "    # 2. Smart Alias Detection\n",
    "    character_exists = False\n",
    "    detected_story = None\n",
    "    canonical_name = character_name\n",
    "    known_aliases = []\n",
    "\n",
    "    for title, chars in global_map.items():\n",
    "        matches = [c for c in chars if character_name.lower() in c.lower()]\n",
    "        if matches:\n",
    "            character_exists = True\n",
    "            detected_story = title\n",
    "            known_aliases = matches\n",
    "            canonical_name = max(matches, key=len)\n",
    "            break\n",
    "\n",
    "    if not character_exists:\n",
    "        return {\"error\": \"Character not found.\"}\n",
    "\n",
    "    # 3. Retrieve (Hybrid)\n",
    "    try:\n",
    "        vector_db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "        dense_retriever = vector_db.as_retriever(search_kwargs={\"k\": 6})\n",
    "        with open(\"bm25_retriever.pkl\", \"rb\") as f:\n",
    "            sparse_retriever = pickle.load(f)\n",
    "            sparse_retriever.k = 6\n",
    "\n",
    "        dense_docs = dense_retriever.invoke(canonical_name)\n",
    "        sparse_docs = sparse_retriever.invoke(canonical_name)\n",
    "        if canonical_name != character_name:\n",
    "             sparse_docs.extend(sparse_retriever.invoke(character_name))\n",
    "    except Exception as e:\n",
    "        dense_docs = []\n",
    "        sparse_docs = []\n",
    "\n",
    "    # Deduplicate & Filter\n",
    "    all_docs = {d.page_content: d for d in sparse_docs + dense_docs}.values()\n",
    "    filtered_docs = [d for d in all_docs if d.metadata.get(\"storyTitle\") == detected_story]\n",
    "\n",
    "    if not filtered_docs:\n",
    "        filtered_docs = list(all_docs)\n",
    "\n",
    "    # 4. Context Preparation\n",
    "    context_text = \"\"\n",
    "    for d in filtered_docs:\n",
    "        local_chars = d.metadata.get(\"chars_in_chunk\", [])\n",
    "        context_text += f\"\"\"\n",
    "        --- Snippet from \"{d.metadata.get('storyTitle')}\" ---\n",
    "        [Characters in this scene: {local_chars}]\n",
    "        Text: {d.page_content}\n",
    "        \"\"\"\n",
    "\n",
    "    # 5. Generation (INFERENCE-ENCOURAGING PROMPT)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a JSON extractor. Output strictly valid JSON matching the provided schema.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Task: Extract character details for \"{canonical_name}\" based ONLY on the text below.\n",
    "\n",
    "        Aliases: The text may refer to them as {known_aliases} or with titles (Mr., Mrs.). Treat them as the same person.\n",
    "\n",
    "        STORY TITLE: \"{detected_story}\"\n",
    "\n",
    "        TEXT CONTEXT:\n",
    "        {context_text}\n",
    "\n",
    "        ### INSTRUCTIONS FOR RELATIONS ###\n",
    "        - Extract clear relationships.\n",
    "        - **IMPORTANT:** If no formal relationship (like \"Father\") is stated, **INFER** the relationship based on interactions.\n",
    "          - Example: If they argue -> \"Adversary\" or \"Conflict\"\n",
    "          - Example: If they work together -> \"Colleague\"\n",
    "          - Example: If they talk -> \"Acquaintance\"\n",
    "\n",
    "        ### REQUIRED OUTPUT FORMAT ###\n",
    "        Example:\n",
    "        {{\n",
    "            \"name\": \"Jon Snow\",\n",
    "            \"storyTitle\": \"A Song of Ice and Fire\",\n",
    "            \"summary\": \"Jon Snow is a brave leader...\",\n",
    "            \"relations\": [\n",
    "                {{ \"name\": \"Arya Stark\", \"relation\": \"Sister\" }},\n",
    "                {{ \"name\": \"Cersei Lannister\", \"relation\": \"Enemy\" }}\n",
    "            ],\n",
    "            \"characterType\": \"Protagonist\"\n",
    "        }}\n",
    "\n",
    "        ### YOUR OUTPUT FOR \"{canonical_name}\" ###\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.complete(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        # DEBUG: Print Raw JSON to see if LLM is generating keys we are deleting\n",
    "        raw_json_str = resp.choices[0].message.content\n",
    "        raw_json = json.loads(raw_json_str)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*20} DEBUG: RAW LLM OUTPUT {'='*20}\")\n",
    "            print(json.dumps(raw_json, indent=2))\n",
    "            print(f\"{'='*60}\\n\")\n",
    "\n",
    "        final_output = clean_and_validate_json(raw_json)\n",
    "        final_output[\"name\"] = canonical_name\n",
    "        final_output[\"storyTitle\"] = detected_story\n",
    "        return final_output\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"LLM Generation Failed: {e}\"}"
   ],
   "id": "4e4cc2643059341b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3420aee2732ecca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:50:04.542415Z",
     "start_time": "2025-11-24T16:50:02.513654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- TEST ---\n",
    "# Set verbose=False to hide the debug prints and only show the final JSON\n",
    "result = get_character_info(\"holohan\", verbose=False)\n",
    "print(json.dumps(result, indent=2))"
   ],
   "id": "68384faabc494b75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Hoppy Holohan\",\n",
      "  \"storyTitle\": \"A Mother\",\n",
      "  \"summary\": \"Hoppy Holohan, assistant secretary of the Eire Abu Society, is responsible for arranging concerts but is assisted by Mrs Kearney. He has a game leg and is known as Hoppy Holohan.\",\n",
      "  \"relations\": [\n",
      "    {\n",
      "      \"name\": \"Mrs Kearney\",\n",
      "      \"relation\": \"Colleague\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Kathleen\",\n",
      "      \"relation\": \"Acquaintance\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Mrs Pat Campbell\",\n",
      "      \"relation\": \"Acquaintance\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"O\\u2019Madden Burke\",\n",
      "      \"relation\": \"Acquaintance\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Fitzpatrick\",\n",
      "      \"relation\": \"Acquaintance\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Meade\",\n",
      "      \"relation\": \"Acquaintance\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Madam Glynn\",\n",
      "      \"relation\": \"Acquaintance\"\n",
      "    }\n",
      "  ],\n",
      "  \"characterType\": \"Supporting\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:49:41.572142Z",
     "start_time": "2025-11-24T16:49:41.483987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!python --version\n",
    "\n"
   ],
   "id": "e42e3a3b85a7cd6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f652f8d6489aafe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
